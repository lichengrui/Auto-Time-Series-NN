{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77dad9e9-8f5a-47e3-a66e-30783efb185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.jit as jit\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "from torchvision import transforms\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818e62db-aecc-4cd2-93c7-7677414d458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size = 32\n",
    "seq_length = 10 \n",
    "input_dim = 5\n",
    "\n",
    "# Generate random time series data (e.g., random walk data)\n",
    "np.random.seed(42)\n",
    "data = np.cumsum(np.random.randn(batch_size, seq_length, input_dim), axis=1)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "import torch\n",
    "input_data = torch.tensor(data, dtype=torch.float32)\n",
    " \n",
    "print(input_data.shape)  # Output: torch.Size([32, 10, 5])\n",
    "input_dim = input_data.shape[2]\n",
    "output_dim = input_data.shape[2]\n",
    "output_seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed92bfd7-7191-4d5f-8c0e-98779d01ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (pos_encoder): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=5, bias=True)\n",
      "  )\n",
      "  (time2vec): Time2Vec(\n",
      "    (linear): Linear(in_features=5, out_features=1, bias=True)\n",
      "    (periodic): Linear(in_features=5, out_features=4, bias=True)\n",
      "  )\n",
      "  (fc_last): Linear(in_features=128, out_features=25, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    142\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_data, batch_targets \u001b[38;5;129;01min\u001b[39;00m input_data:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[0;32m    146\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.periodic = nn.Linear(input_dim, output_dim-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        linear_part = self.linear(x)\n",
    "        periodic_part = torch.sin(self.periodic(x))\n",
    "        return torch.cat([linear_part, periodic_part], dim=-1)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)\n",
    "\n",
    "class FeaturePyramid(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.down1 = nn.Linear(input_dim, input_dim//2)\n",
    "        self.down2 = nn.Linear(input_dim//2, input_dim//4)\n",
    "        self.up1 = nn.Linear(input_dim//4, input_dim//2)\n",
    "        self.up2 = nn.Linear(input_dim//2, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        u1 = self.up1(d2)\n",
    "        u2 = self.up2(u1 + d1)\n",
    "        return u2 + x\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, output_seq_len):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Create ModuleList to store layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Conditional addition of positional encoding\n",
    "        if random.random() < 0.5:\n",
    "            self.pos_encoder = nn.Sequential(\n",
    "                nn.Linear(1, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, input_dim)\n",
    "            )\n",
    "            self.use_pos_encoding = True\n",
    "        else:\n",
    "            self.use_pos_encoding = False\n",
    "            \n",
    "        # Conditional addition of Time2Vec\n",
    "        if random.random() < 0.5:\n",
    "            self.time2vec = Time2Vec(input_dim, input_dim)\n",
    "            self.use_time2vec = True\n",
    "        else:\n",
    "            self.use_time2vec = False\n",
    "            \n",
    "        # Conditional addition of multi-scale processing\n",
    "        if random.random() < 0.5:\n",
    "            self.scales = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, input_dim)\n",
    "                ) for _ in range(3)\n",
    "            ])\n",
    "            self.use_multiscale = True\n",
    "        else:\n",
    "            self.use_multiscale = False\n",
    "            \n",
    "        # Base layers\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        # Conditional addition of ResidualBlock\n",
    "        if random.random() < 0.5:\n",
    "            self.layers.append(ResidualBlock(hidden_dim))\n",
    "            \n",
    "        # Conditional addition of FeaturePyramid\n",
    "        if random.random() < 0:\n",
    "            self.layers.append(FeaturePyramid(hidden_dim))\n",
    "        \n",
    "        # Final output layer\n",
    "        self.fc_last = nn.Linear(hidden_dim, output_dim * output_seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Apply positional encoding if selected\n",
    "        if self.use_pos_encoding:\n",
    "            positions = torch.arange(seq_len, device=x.device).unsqueeze(-1).float()\n",
    "            pos_encoding = self.pos_encoder(positions)\n",
    "            x = x + pos_encoding.unsqueeze(0)\n",
    "            \n",
    "        # Apply Time2Vec if selected\n",
    "        if self.use_time2vec:\n",
    "            x = self.time2vec(x)\n",
    "            \n",
    "        # Flatten for processing\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        # Apply multi-scale processing if selected\n",
    "        if self.use_multiscale:\n",
    "            scale_outputs = []\n",
    "            for scale_layer in self.scales:\n",
    "                scale_outputs.append(scale_layer(x))\n",
    "            x = torch.mean(torch.stack(scale_outputs), dim=0)\n",
    "        \n",
    "        # Process through main layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Final processing\n",
    "        x = self.fc_last(x)\n",
    "        x = x.view(batch_size, -1, self.output_dim)\n",
    "        x = x[:, :self.output_seq_len, :]\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Usage remains the same\n",
    "hidden_dim = 128\n",
    "model = SimpleNN(input_dim, hidden_dim, output_dim, output_seq_len)\n",
    "print(model)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_data, batch_targets in input_data:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_data)  # (batch_size, output_seq_len, output_dim)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971a2dc-5807-4012-9597-213e904c3300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
